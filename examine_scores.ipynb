{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1,76.31,0.727,0.819,0.576,$10.43\n",
      "claude-sonnet-4,81.99,0.726,0.844,0.818,$18.73\n",
      "gemini-2.5-pro,72.06,0.675,0.747,0.662,$52.90\n",
      "openrouter/horizon-alpha,82.25,0.804,0.85,0.733,\n",
      "mistralai/Mistral-Small-3.2-24B-Instruct-2506,50.21,0.396,0.578,0.306,$0.49\n",
      "qwen/qwen3-235b-a22b-2507,59.69,0.437,0.61,0.704,$0.94\n",
      "z-ai/glm-4.5,63.41,0.53,0.664,0.619,$4.40\n",
      "gemini-2.5-flash,62.97,0.532,0.66,0.605,$1.87\n",
      "openrouter/horizon-beta,82.38,0.819,0.844,0.747,\n",
      "o3,75.1,0.699,0.779,0.69,$16.16\n",
      "gpt-5-mini-2025-08-07:minimal-reasoning,74.49,0.694,0.778,0.662,$1.49\n",
      "gpt-5-nano-2025-08-07:minimal-reasoning,52.2,0.411,0.536,0.576,$0.29\n",
      "openai/gpt-oss-120b,54.78,0.332,0.57,0.676,$1.14\n",
      "openai/gpt-oss-20b,42.8,0.331,0.476,0.334,$0.45\n",
      "gpt-5-2025-08-07:minimal-reasoning,81.2,0.8,0.838,0.719,$7.43\n",
      "gpt-5-nano-2025-08-07__5x-ensemble,61.08,0.579,0.635,0.548,$1.45\n",
      "moonshotai/Kimi-K2-Instruct,73.09,0.66,0.752,0.719,$3.70\n",
      "qwen/qwen3-30b-a3b-instruct-2507,34.58,0.153,0.343,0.548,$0.98\n",
      "gemini-2.5-flash-lite,55.99,0.483,0.607,0.448,$0.50\n",
      "gpt-4.1-mini,57.26,0.395,0.623,0.548,$2.09\n",
      "mistral-medium-3,58.53,0.496,0.621,0.533,$2.31\n",
      "qwen/qwen3-235b-a22b,56.18,0.318,0.616,0.59,$1.00\n",
      "o4-mini,56.1,0.455,0.594,0.533,$11.81\n",
      "meta-llama/llama-3.1-8b-instruct,19.18,0.0,0.286,0.006,$0.11\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from config.constants import MODEL_NAME_REPLACEMENTS\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Paths\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path(\".\").resolve()\n",
    "RESULTS_DIR = HERE / \"results\"\n",
    "OUTPUT_JSON_DIR = RESULTS_DIR / \"stats\"\n",
    "OUTPUT_CSV = RESULTS_DIR / \"scores.csv\"\n",
    "RUNS_DIR = (HERE / \"v3_results_x96\").resolve()\n",
    "\n",
    "OUTPUT_JSON_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Costs keyed by *exact* judge model string used in the run, where lists are\n",
    "# joined by \",\" (e.g., \"a,b,c\"). Unknowns = None.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "run_cost_usd = {\n",
    "    \"google/gemini-2.5-flash-lite-preview-06-17\": 0.53,\n",
    "    \"google/gemini-2.5-flash\": 1.87,\n",
    "    \"moonshotai/kimi-k2\": 3.70,\n",
    "    \"openai/gpt-4.1-mini\": 2.09,\n",
    "    \"mistralai/mistral-medium-3\": 2.31,\n",
    "    \"qwen/qwen3-235b-a22b\": 1.00,\n",
    "    \"o3\": 16.16,  # also used in run 23 where cost was 13.00 for that run; keep model-level as per provided list\n",
    "    \"o4-mini\": 11.81,\n",
    "    \"meta-llama/llama-3.1-8b-instruct\": 0.11,\n",
    "    \"openai/gpt-4.1\": 10.43,\n",
    "    \"anthropic/claude-3.7-sonnet\": None,\n",
    "    \"anthropic/claude-sonnet-4\": 18.73,\n",
    "    \"google/gemini-2.5-pro\": 52.90,  # 40.57 - (-12.33)\n",
    "    #\"openrouter/horizon-alpha,openrouter/horizon-alpha,openrouter/horizon-alpha,openrouter/horizon-alpha,openrouter/horizon-alpha\": None,\n",
    "    \"openrouter/horizon-alpha\": None,\n",
    "    \"mistralai/mistral-small-3.2-24b-instruct\": 0.49,\n",
    "    \"qwen/qwen3-235b-a22b-2507\": 0.94,\n",
    "    \"z-ai/glm-4.5\": 4.40,\n",
    "    #\"mistralai/mistral-small-3.2-24b-instruct,mistralai/mistral-small-3.2-24b-instruct,mistralai/mistral-small-3.2-24b-instruct,mistralai/mistral-small-3.2-24b-instruct,mistralai/mistral-small-3.2-24b-instruct\": 2.37,\n",
    "    #\"moonshotai/kimi-k2,openrouter/horizon-beta\": None,\n",
    "    #\"openrouter/horizon-beta,openrouter/horizon-beta,openrouter/horizon-beta\": None,\n",
    "    \"openrouter/horizon-beta\": None,\n",
    "    # Separate run noted with cost 13.00 for o3 in a different prompt variant:\n",
    "    # If you prefer per-run specificity, you'd need per-file mapping instead of model-keyed mapping.\n",
    "    \"gpt-5-mini-2025-08-07\": 1.49,\n",
    "    \"gpt-5-nano-2025-08-07\": 0.29,\n",
    "    \"gpt-5-nano-2025-08-07,gpt-5-nano-2025-08-07,gpt-5-nano-2025-08-07,gpt-5-nano-2025-08-07,gpt-5-nano-2025-08-07\": 1.45,\n",
    "    \"openai/gpt-oss-120b\": 1.14,\n",
    "    \"openai/gpt-oss-20b\": 0.45,\n",
    "    \"gpt-5-2025-08-07\": 7.43,\n",
    "    \"qwen/qwen3-30b-a3b-instruct-2507\": 0.98,\n",
    "    \"google/gemini-2.5-flash-lite\": 0.5,\n",
    "\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Optional ignores\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "runids_to_ignore = set()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CSV header\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "csv_header = [\n",
    "    \"model\",\n",
    "    \"judgemark_score\",\n",
    "    \"stability\",\n",
    "    \"separability\",\n",
    "    \"human_corr\",\n",
    "    \"cost\"\n",
    "]\n",
    "\n",
    "def normalize_model_name(name: str) -> str:\n",
    "    if not name:\n",
    "        return name\n",
    "    return MODEL_NAME_REPLACEMENTS.get(name, name)\n",
    "\n",
    "def fmt_cost(val) -> str:\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"${float(val):.2f}\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_float(x, default=None):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def extract_model_fields(judge_data: dict):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - model_for_csv: normalized single-name for CSV display (first model if list)\n",
    "      - cost_lookup_key: exact joined string for cost lookup (list joined with ',')\n",
    "    \"\"\"\n",
    "    field_models = judge_data.get(\"judge_models\")\n",
    "    field_model  = judge_data.get(\"judge_model\")\n",
    "\n",
    "    # Build cost lookup key\n",
    "    if isinstance(field_models, list):\n",
    "        cost_lookup_key = \",\".join(field_models)\n",
    "    elif isinstance(field_model, str):\n",
    "        cost_lookup_key = field_model\n",
    "    else:\n",
    "        cost_lookup_key = \"\"\n",
    "\n",
    "    # Model name to show in CSV (normalize first entry)\n",
    "    if isinstance(field_models, list) and field_models:\n",
    "        model_for_csv = normalize_model_name(','.join(field_models))\n",
    "    elif isinstance(field_model, str):\n",
    "        model_for_csv = normalize_model_name(field_model.split(\",\")[0].strip())\n",
    "    else:\n",
    "        model_for_csv = \"\"\n",
    "\n",
    "    return model_for_csv, cost_lookup_key\n",
    "\n",
    "def compute_row(judge_data: dict, fallback_cost: float | None):\n",
    "    \"\"\"\n",
    "    Build a CSV row + return a sanitized copy of judge_data for JSON dump.\n",
    "    \"\"\"\n",
    "    # Names\n",
    "    judge_model_name, _ = extract_model_fields(judge_data)\n",
    "\n",
    "    # Core score\n",
    "    judgemark_score_raw = judge_data.get(\"final_judgemark_score_raw\")\n",
    "\n",
    "    # Elements for components (raw version)\n",
    "    norm_stats = judge_data.get(\"final_judgemark_score_elements_raw\", {}) or {}\n",
    "\n",
    "    stability = norm_stats.get(\"norm_stability_between_iterations\")\n",
    "\n",
    "    # Separability = avg(norm_kruskall_wallis, norm_ci99_adjacent_overlap)\n",
    "    norm_kw = safe_float(norm_stats.get(\"norm_kruskall_wallis\", 0.0), 0.0)\n",
    "    norm_ci = safe_float(norm_stats.get(\"norm_ci99_adjacent_overlap\", 0.0), 0.0)\n",
    "    separability = (norm_kw + norm_ci) / 2.0 if (norm_kw is not None and norm_ci is not None) else None\n",
    "\n",
    "    human_corr = norm_stats.get(\"norm_correlation_with_lmsys_arena\")\n",
    "\n",
    "    # Prefer directly stored billing if present; else fall back to our lookup\n",
    "    cost_candidates = [\n",
    "        judge_data.get(\"billing\", {}).get(\"total_usd\"),\n",
    "        judge_data.get(\"billing_total_usd\"),\n",
    "        judge_data.get(\"cost_usd\"),\n",
    "    ]\n",
    "    cost_val = next((c for c in cost_candidates if isinstance(c, (int, float, str)) and str(c).strip() != \"\"), None)\n",
    "    if cost_val is None:\n",
    "        cost_val = fallback_cost\n",
    "\n",
    "    # Row\n",
    "    row = [\n",
    "        judge_model_name,\n",
    "        round(100 * judgemark_score_raw, 2) if isinstance(judgemark_score_raw, (int, float)) else \"\",\n",
    "        round(float(stability), 3) if isinstance(stability, (int, float)) else \"\",\n",
    "        round(float(separability), 3) if isinstance(separability, (int, float)) else \"\",\n",
    "        round(float(human_corr), 3) if isinstance(human_corr, (int, float)) else \"\",\n",
    "        fmt_cost(cost_val)\n",
    "    ]\n",
    "\n",
    "    # Sanitize JSON copy (drop heavy 'results' if present)\n",
    "    jd_copy = dict(judge_data)\n",
    "    if \"results\" in jd_copy:\n",
    "        try:\n",
    "            del jd_copy[\"results\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return row, jd_copy\n",
    "\n",
    "def safe_filename_from_model(model_name: str) -> str:\n",
    "    s = (model_name or \"\").replace(\"/\", \"__\")\n",
    "    return re.sub(r\"[^\\w\\-]\", \"-\", s)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Walk all run files and aggregate\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "csv_rows = []\n",
    "json_written = 0\n",
    "\n",
    "# Gather files like 1.json, 2.json, ... plus any stragglers\n",
    "run_files = sorted([Path(p) for p in glob(str(RUNS_DIR / \"*.json\"))])\n",
    "\n",
    "for path in run_files:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        payload = json.load(fh)\n",
    "\n",
    "    # Expect shape: { run_id: judge_data, ... } — tolerate other shapes\n",
    "    if isinstance(payload, dict):\n",
    "        items = payload.items()\n",
    "    elif isinstance(payload, list):\n",
    "        items = enumerate(payload)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for run_id, judge_data in items:\n",
    "        if run_id in runids_to_ignore:\n",
    "            continue\n",
    "        if not isinstance(judge_data, dict):\n",
    "            continue\n",
    "\n",
    "        # NEW: derive fallback cost by reading judge_models/judge_model\n",
    "        _, cost_key = extract_model_fields(judge_data)\n",
    "        fallback_cost = run_cost_usd.get(cost_key, None)\n",
    "\n",
    "        row, jd_copy = compute_row(judge_data, fallback_cost)\n",
    "        csv_rows.append(row)\n",
    "\n",
    "        # Write a per-judge JSON (model-normalized filename)\n",
    "        model_name = row[0]\n",
    "        safe_stem = safe_filename_from_model(model_name)\n",
    "        json_output_path = OUTPUT_JSON_DIR / f\"{safe_stem}.json\"\n",
    "\n",
    "        with json_output_path.open(\"w\", encoding=\"utf-8\") as outfh:\n",
    "            json.dump(jd_copy, outfh, indent=4)\n",
    "            json_written += 1\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Write CSV\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with OUTPUT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(csv_header)\n",
    "    csv_writer.writerows(csv_rows)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Echo CSV rows as lines for quick eyeballing\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "raw = \"\"\n",
    "for row in csv_rows:\n",
    "    row_str = \",\".join(map(str, row))\n",
    "    raw += \"\\n\" + row_str\n",
    "    print(row_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark mode plot saved to model_pareto_plot_dark.html\n",
      "\n",
      "Filtered DataFrame (Cost > 0):\n",
      "                                            model  score   cost\n",
      "0                                         gpt-4.1  76.31  10.43\n",
      "1                                 claude-sonnet-4  81.99  18.73\n",
      "2                                  gemini-2.5-pro  72.06  52.90\n",
      "4   mistralai/Mistral-Small-3.2-24B-Instruct-2506  50.21   0.49\n",
      "5                       qwen/qwen3-235b-a22b-2507  59.69   0.94\n",
      "6                                    z-ai/glm-4.5  63.41   4.40\n",
      "7                                gemini-2.5-flash  62.97   1.87\n",
      "9                                              o3  75.10  16.16\n",
      "10        gpt-5-mini-2025-08-07:minimal-reasoning  74.49   1.49\n",
      "11        gpt-5-nano-2025-08-07:minimal-reasoning  52.20   0.29\n",
      "12                            openai/gpt-oss-120b  54.78   1.14\n",
      "13                             openai/gpt-oss-20b  42.80   0.45\n",
      "14             gpt-5-2025-08-07:minimal-reasoning  81.20   7.43\n",
      "15             gpt-5-nano-2025-08-07__5x-ensemble  61.08   1.45\n",
      "16                    moonshotai/Kimi-K2-Instruct  73.09   3.70\n",
      "17               qwen/qwen3-30b-a3b-instruct-2507  34.58   0.98\n",
      "18                          gemini-2.5-flash-lite  55.99   0.50\n",
      "19                                   gpt-4.1-mini  57.26   2.09\n",
      "20                               mistral-medium-3  58.53   2.31\n",
      "21                           qwen/qwen3-235b-a22b  56.18   1.00\n",
      "22                                        o4-mini  56.10  11.81\n",
      "23               meta-llama/llama-3.1-8b-instruct  19.18   0.11\n",
      "\n",
      "Models on the Pareto Frontier:\n",
      "                                  model  score  cost\n",
      "       meta-llama/llama-3.1-8b-instruct  19.18  0.11\n",
      "gpt-5-nano-2025-08-07:minimal-reasoning  52.20  0.29\n",
      "                  gemini-2.5-flash-lite  55.99  0.50\n",
      "              qwen/qwen3-235b-a22b-2507  59.69  0.94\n",
      "     gpt-5-nano-2025-08-07__5x-ensemble  61.08  1.45\n",
      "gpt-5-mini-2025-08-07:minimal-reasoning  74.49  1.49\n",
      "     gpt-5-2025-08-07:minimal-reasoning  81.20  7.43\n",
      "                        claude-sonnet-4  81.99 18.73\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import io # To read the string data\n",
    "\n",
    "# Raw data provided by the user\n",
    "\n",
    "# 1. Parse Data\n",
    "# Parse Data\n",
    "rows = []\n",
    "for line in raw.splitlines():\n",
    "    line = line.lstrip('*') # remove leading *\n",
    "    parts = line.split(',')\n",
    "    if len(parts) < 2: continue # Skip empty or malformed lines\n",
    "\n",
    "    model = parts[0].strip()\n",
    "    try:\n",
    "        score = float(parts[1])\n",
    "    except ValueError:\n",
    "        score = np.nan # Handle cases where score might be missing/invalid\n",
    "\n",
    "    cost = np.nan\n",
    "    # Only look for a cost if the last element isn't an empty string\n",
    "    if parts[-1].strip():\n",
    "        # Search for cost from the end\n",
    "        for part in reversed(parts):\n",
    "            part = part.strip()\n",
    "            if part.startswith('$'):\n",
    "                try:\n",
    "                    cost = float(part[1:])\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            elif part == '': # Handle trailing comma case\n",
    "                continue\n",
    "            else:\n",
    "                try: # Handle cost without $ sign\n",
    "                    cost = float(part)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    rows.append({'model': model, 'score': score, 'cost': cost})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Drop rows where score or cost couldn't be parsed\n",
    "df.dropna(subset=['score', 'cost'], inplace=True)\n",
    "\n",
    "# 2. Filter Data: Remove items with cost 0\n",
    "# 2. Filter Data: keep only finite, positive costs (log axis needs >0)\n",
    "df_filtered = df[(df['cost'] > 0) & np.isfinite(df['cost'])].copy()\n",
    "\n",
    "\n",
    "# 3. Compute Pareto frontier on filtered data\n",
    "frontier_indices = []\n",
    "is_frontier = pd.Series([False] * len(df_filtered), index=df_filtered.index)\n",
    "\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    s = row['score']\n",
    "    c = row['cost']\n",
    "    other_points = df_filtered.drop(index=idx)\n",
    "    dominated = (\n",
    "        (other_points['score'] >= s) &\n",
    "        (other_points['cost'] <= c) &\n",
    "        ((other_points['score'] > s) | (other_points['cost'] < c))\n",
    "    ).any()\n",
    "\n",
    "    if not dominated:\n",
    "        frontier_indices.append(idx)\n",
    "        is_frontier[idx] = True\n",
    "\n",
    "df_frontier = df_filtered.loc[frontier_indices].sort_values('cost')\n",
    "\n",
    "# 4. Prepare for Plotting & Define Adjustments\n",
    "adjustments = {\n",
    "    \n",
    "    'gpt-4.1-mini': 0.0,\n",
    "    'gpt-4o-mini': 0.0,           \n",
    "    \n",
    "    'qwen/qwen3-32b': 0.0,\n",
    "    'qwen/qwen3-14b': 0.0,\n",
    "    \"qwen/qwen3-235b-a22b\": 0.0,\n",
    "    #\"qwen/qwen3-235b-a22b:thinking\": -1.1,\n",
    "    \"qwen/qwen3-30b-a3b\": 0.0,    \n",
    "    \n",
    "    'mistral-medium-3': 0.0,    \n",
    "    'o3:low-reasoning': 0.0,\n",
    "    'mistralai/Mistral-Small-3.2-24B-Instruct-2506': 0,\n",
    "    'gemini-2.5-flash': -0.4,\n",
    "    'gpt-5-2025-08-07:minimal-reasoning': -0.3,\n",
    "    'openai/gpt-oss-120b': -0.5,\n",
    "    'gemini-2.5-flash-lite': -0.6,\n",
    "\n",
    "}\n",
    "base_pixel_shift = 10 # Base shift in pixels\n",
    "\n",
    "# 5. Create Plot with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define colors suitable for dark mode\n",
    "non_frontier_color = 'rgba(135, 206, 250, 0.8)' # Light Sky Blue\n",
    "frontier_marker_color = 'rgba(255, 105, 97, 1.0)' # Pastel Red / Coral\n",
    "frontier_line_color = 'rgba(255, 105, 97, 0.8)'\n",
    "frontier_marker_border_color = 'lightgrey' # Light grey border for diamond\n",
    "annotation_font_color = 'white' # White text for dark background\n",
    "\n",
    "# Add all non-frontier points first\n",
    "df_non_frontier = df_filtered[~is_frontier]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_non_frontier['cost'],\n",
    "    y=df_non_frontier['score'],\n",
    "    mode='markers',\n",
    "    marker=dict(color=non_frontier_color, size=8),\n",
    "    name='Other Models',\n",
    "    text=df_non_frontier['model'], # Text for hover\n",
    "    hoverinfo='text+x+y'\n",
    "))\n",
    "\n",
    "# Add frontier points (different marker)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_frontier['cost'],\n",
    "    y=df_frontier['score'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        symbol='diamond',\n",
    "        size=12,\n",
    "        color=frontier_marker_color,\n",
    "        line=dict(width=1, color=frontier_marker_border_color) # Use light border\n",
    "    ),\n",
    "    name='Pareto Frontier',\n",
    "    text=df_frontier['model'], # Text for hover\n",
    "    hoverinfo='text+x+y'\n",
    "))\n",
    "\n",
    "# Add line connecting frontier points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_frontier['cost'],\n",
    "    y=df_frontier['score'],\n",
    "    mode='lines',\n",
    "    line=dict(color=frontier_line_color, width=2, dash='dash'),\n",
    "    name='Frontier Line',\n",
    "    hoverinfo='skip' # Don't show hover for the line itself\n",
    "))\n",
    "\n",
    "# Labels rendered as a text trace so they stick to data coords on log x\n",
    "# Labels rendered as a text trace so they stick to data coords on log x\n",
    "label_texts = []\n",
    "label_positions = []\n",
    "for m, c in zip(df_filtered['model'], df_filtered['cost']):\n",
    "    label_texts.append(f\"{m} (${c:.2f})\")\n",
    "    if m.strip().lower() == \"gemini-2.5-pro\":\n",
    "        label_positions.append(\"middle left\")\n",
    "    elif m.strip().lower() == \"gemini-2.5-flash-lite\":\n",
    "        label_positions.append(\"top right\")\n",
    "    else:\n",
    "        label_positions.append(\"middle right\")\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_filtered['cost'],\n",
    "    y=df_filtered['score'],\n",
    "    mode='text',\n",
    "    text=label_texts,\n",
    "    textposition=label_positions,  # array for per-point control\n",
    "    textfont=dict(size=9, color=annotation_font_color),\n",
    "    showlegend=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "# Update layout for dark mode\n",
    "fig.update_layout(\n",
    "    title='Judgemark Score vs Cost',\n",
    "    xaxis_title='Cost ($) to complete benchmark',\n",
    "    yaxis_title='Judgemark Score (Performance as LLM Judge)',\n",
    "    template='plotly_dark',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ),\n",
    "    margin=dict(l=40, r=40, t=80, b=40),\n",
    "    height=700,\n",
    "    width=1000,\n",
    "    hovermode='closest',\n",
    "    xaxis=dict(\n",
    "        type='log',  # <-- This makes the x-axis logarithmic\n",
    "        gridcolor='rgba(80,80,80,0.5)'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        gridcolor='rgba(80,80,80,0.5)'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "xmin = df_filtered['cost'].min()\n",
    "xmax = df_filtered['cost'].max()\n",
    "logpad = 0.15  # 15% padding in log space\n",
    "\n",
    "# Build “1–2–5” log ticks within [xmin, xmax], label as normal currency.\n",
    "xmin = df_filtered['cost'].min()\n",
    "xmax = df_filtered['cost'].max()\n",
    "logpad = 0.15\n",
    "\n",
    "# candidate decades\n",
    "decades = np.arange(np.floor(np.log10(xmin)), np.ceil(np.log10(xmax)) + 1)\n",
    "bases = np.array([1.0, 2.0, 5.0])\n",
    "tickvals = []\n",
    "for d in decades:\n",
    "    tickvals.extend(list(bases * (10.0 ** d)))\n",
    "tickvals = [v for v in tickvals if (v >= xmin / (10**logpad)) and (v <= xmax * (10**logpad))]\n",
    "\n",
    "def fmt_money(v: float) -> str:\n",
    "    # 0 < v < 1 → 2–3 decimals, otherwise up to 2 decimals, no SI prefixes\n",
    "    if v < 1:\n",
    "        s = f\"${v:.3f}\".rstrip('0').rstrip('.')\n",
    "    else:\n",
    "        s = f\"${v:.2f}\".rstrip('0').rstrip('.')\n",
    "    return s\n",
    "\n",
    "ticktext = [fmt_money(v) for v in tickvals]\n",
    "\n",
    "fig.update_xaxes(\n",
    "    type='log',\n",
    "    range=[np.log10(xmin) - logpad, np.log10(xmax) + logpad],\n",
    "    tickmode='array',\n",
    "    tickvals=tickvals,\n",
    "    ticktext=ticktext,\n",
    "    showexponent='none',          # don’t show 10^n anywhere\n",
    "    gridcolor='rgba(80,80,80,0.5)'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    gridcolor='rgba(80,80,80,0.5)'\n",
    ")\n",
    "\n",
    "\n",
    "# Save to HTML and print confirmation + data\n",
    "html_file = 'model_pareto_plot_dark.html' # Changed filename\n",
    "fig.write_html(html_file)\n",
    "\n",
    "print(f\"Dark mode plot saved to {html_file}\")\n",
    "print(\"\\nFiltered DataFrame (Cost > 0):\")\n",
    "print(df_filtered.to_string())\n",
    "print(\"\\nModels on the Pareto Frontier:\")\n",
    "print(df_frontier[['model', 'score', 'cost']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
